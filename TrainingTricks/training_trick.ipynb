{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    x = tf.reshape(x, [28*28])\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "(x, y), (x_test, y_test) = datasets.mnist.load_data()  # 得到训练集和测试集\n",
    "# 认为划分数据集\n",
    "x_train, x_val = tf.split(x, num_or_size_splits=[50000, 10000])\n",
    "y_train, y_val = tf.split(y, num_or_size_splits=[50000, 10000])\n",
    "\n",
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_train = db_train.map(preprocess).shuffle(50000).batch(batch_size)\n",
    "\n",
    "db_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "db_val = db_val.map(preprocess).shuffle(10000).batch(batch_size)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.map(preprocess).batch(batch_size)  # 一般不会打乱测试数据的顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  330       \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = Sequential([layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(128, activation='relu'),\n",
    "                     layers.Dense(64, activation='relu'),\n",
    "                     layers.Dense(32, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.build(input_shape=(None, 28*28))\n",
    "network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.1850 - accuracy: 0.9618\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1648 - accuracy: 0.9644 - val_loss: 0.1869 - val_accuracy: 0.9630\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.1513 - accuracy: 0.9676\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1557 - accuracy: 0.9678 - val_loss: 0.1687 - val_accuracy: 0.9634\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1551 - accuracy: 0.9674\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1955 - accuracy: 0.9559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19548417586917297, 0.9559]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.compile(optimizer=optimizers.Adam(lr=0.01), \n",
    "                loss=tf.losses.CategoricalCrossentropy(from_logits=True), \n",
    "                metrics=['accuracy'])\n",
    "network.fit(db_train, epochs=5, validation_data=db_val, validation_freq=2)\n",
    "network.evaluate(db_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(500):\n",
    "    idx = tf.random.shuffle(tf.range(60000))\n",
    "    x_train, y_train = tf.gather(x, idx[:50000]), tf.gather(y, id[:50000])\n",
    "    x_val, y_val = tf.gather(x, idx[-10000:]), tf.gather(y, idx[-10000:])\n",
    "    \n",
    "    db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    db_train = db_train.map(preprocess).shuffle(50000).batch(batch_size)\n",
    "\n",
    "    db_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    db_val = db_val.map(preprocess).shuffle(10000).batch(batch_size)\n",
    "    # training\n",
    "    \n",
    "    # validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "network.fit(db_all, epochs=500, validation_split=0.1, validation_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "l2_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), \n",
    "                       activation=tf.nn.relu, input_shape=(num_words, )),\n",
    "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), \n",
    "                       activation=tf.nn.relu, input_shape=(num_words, )),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "for step, (x,y) in enumerate(db):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # [b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        # [b, 784] => [b, 10]\n",
    "        out = network(x)\n",
    "        # [b] => [b, 10]\n",
    "        y_onehot = tf.one_hot(y, depth=10) \n",
    "        # [b]\n",
    "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n",
    "\n",
    "\n",
    "        loss_regularization = []\n",
    "        for p in network.trainable_variables:\n",
    "            loss_regularization.append(tf.nn.l2_loss(p))\n",
    "        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n",
    "\n",
    "        loss = loss + 0.0001 * loss_regularization\n",
    " \n",
    "\n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = rl = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
